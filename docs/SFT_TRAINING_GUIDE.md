# SFT (Supervised Fine-Tuning) å®Œå…¨ã‚¬ã‚¤ãƒ‰

## ğŸ“š ç›®æ¬¡
1. [æ¦‚è¦](#æ¦‚è¦)
2. [Hugging Faceãƒ¢ãƒ‡ãƒ«ã®æº–å‚™](#hugging-faceãƒ¢ãƒ‡ãƒ«ã®æº–å‚™)
3. [ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™](#ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™)
4. [SFTè¨“ç·´ã®å®Ÿè¡Œ](#sftè¨“ç·´ã®å®Ÿè¡Œ)
5. [ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ](#ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ)
6. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)

## æ¦‚è¦

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€H200 GPUã‚¯ãƒ©ã‚¹ã‚¿ã§å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®SFTï¼ˆæ•™å¸«ã‚ã‚Šå¾®èª¿æ•´ï¼‰ã‚’å®Ÿè¡Œã™ã‚‹å®Œå…¨ãªæ‰‹é †ã‚’èª¬æ˜ã—ã¾ã™ã€‚

### SFTã¨ã¯ï¼Ÿ
- **Supervised Fine-Tuning**: äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³ã«é©å¿œã•ã›ã‚‹æ‰‹æ³•
- **ãƒ¡ãƒªãƒƒãƒˆ**: å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§é«˜å“è³ªãªã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆå¯èƒ½
- **ç”¨é€”**: ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã€ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã€å°‚é–€åˆ†é‡ã®è³ªå•å¿œç­”ãªã©

## Hugging Faceãƒ¢ãƒ‡ãƒ«ã®æº–å‚™

### Step 1: Hugging Faceãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—

1. **ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆ**
   - https://huggingface.co/join ã«ã‚¢ã‚¯ã‚»ã‚¹
   - ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ä½œæˆï¼ˆç„¡æ–™ï¼‰

2. **ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ**
   - https://huggingface.co/settings/tokens ã«ã‚¢ã‚¯ã‚»ã‚¹
   - "New token"ã‚’ã‚¯ãƒªãƒƒã‚¯
   - Token name: `h200-benchmark`
   - Role: `read`ã‚’é¸æŠ
   - "Generate token"ã‚’ã‚¯ãƒªãƒƒã‚¯
   - ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆä¸€åº¦ã ã‘è¡¨ç¤ºã•ã‚Œã¾ã™ï¼‰

3. **ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š**
   ```bash
   # è‡ªå‹•è¨­å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
   ./setup/configure_huggingface.sh
   
   # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è²¼ã‚Šä»˜ã‘
   # Enter your Hugging Face token: hf_xxxxxxxxxxxxx
   ```

### Step 2: Llama-2ãƒ¢ãƒ‡ãƒ«ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ç”³è«‹ï¼ˆå¿…è¦ãªå ´åˆï¼‰

Llama-2ãªã©ã®ã‚²ãƒ¼ãƒˆä»˜ããƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆï¼š

1. **ãƒ¢ãƒ‡ãƒ«ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹**
   - https://huggingface.co/meta-llama/Llama-2-7b-hf
   - "Request access"ã‚’ã‚¯ãƒªãƒƒã‚¯

2. **åˆ©ç”¨è¦ç´„ã«åŒæ„**
   - Metaç¤¾ã®åˆ©ç”¨è¦ç´„ã‚’ç¢ºèª
   - å¿…è¦äº‹é …ã‚’å…¥åŠ›
   - Submit

3. **æ‰¿èªå¾…ã¡**
   - é€šå¸¸24æ™‚é–“ä»¥å†…ã«æ‰¿èª
   - ãƒ¡ãƒ¼ãƒ«ã§é€šçŸ¥ãŒå±Šãã¾ã™

### Step 3: åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«

#### å…¬é–‹ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ä¸è¦ï¼‰
```python
# ã™ãã«ä½¿ãˆã‚‹ãƒ¢ãƒ‡ãƒ«
models = [
    "gpt2",                          # 124M params
    "EleutherAI/gpt-neo-125M",      # 125M params
    "microsoft/phi-2",               # 2.7B params
    "mistralai/Mistral-7B-v0.1",   # 7B params
    "tiiuae/falcon-7b",            # 7B params
]
```

#### ã‚²ãƒ¼ãƒˆä»˜ããƒ¢ãƒ‡ãƒ«ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³å¿…è¦ï¼‰
```python
# ã‚¢ã‚¯ã‚»ã‚¹ç”³è«‹ãŒå¿…è¦
gated_models = [
    "meta-llama/Llama-2-7b-hf",    # 7B params
    "meta-llama/Llama-2-13b-hf",   # 13B params
    "meta-llama/Llama-2-70b-hf",   # 70B params
    "google/gemma-7b",              # 7B params
]
```

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™

### æ–¹æ³•1: å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨

```bash
# Alpacaãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæ¨å¥¨ï¼‰
python3 datasets/prepare_custom_dataset.py \
    --model meta-llama/Llama-2-7b-hf \
    --source alpaca \
    --num-samples 10000 \
    --format alpaca

# Dollyãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
python3 datasets/prepare_custom_dataset.py \
    --model meta-llama/Llama-2-7b-hf \
    --source dolly \
    --num-samples 5000 \
    --format alpaca

# WizardLMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
python3 datasets/prepare_custom_dataset.py \
    --model meta-llama/Llama-2-7b-hf \
    --source wizardlm \
    --num-samples 20000 \
    --format alpaca
```

### æ–¹æ³•2: ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨

#### JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
```json
// data.json
[
    {
        "instruction": "æ±äº¬ã®äººå£ã‚’æ•™ãˆã¦ãã ã•ã„",
        "input": "",
        "output": "æ±äº¬éƒ½ã®äººå£ã¯ç´„1400ä¸‡äººã§ã™ï¼ˆ2023å¹´æ™‚ç‚¹ï¼‰"
    },
    {
        "instruction": "æ¬¡ã®æ–‡ç« ã‚’è¦ç´„ã—ã¦ãã ã•ã„",
        "input": "äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ã¯...",
        "output": "AIã¯äººé–“ã®çŸ¥èƒ½ã‚’æ¨¡å€£ã™ã‚‹..."
    }
]
```

```bash
# ã‚«ã‚¹ã‚¿ãƒ JSONãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
python3 datasets/prepare_custom_dataset.py \
    --model meta-llama/Llama-2-7b-hf \
    --source json \
    --source-path ./data.json \
    --format alpaca
```

#### CSVãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
```csv
instruction,input,output
"æ±äº¬ã®äººå£ã‚’æ•™ãˆã¦ãã ã•ã„","","æ±äº¬éƒ½ã®äººå£ã¯ç´„1400ä¸‡äººã§ã™"
"æ¬¡ã®æ–‡ç« ã‚’è¦ç´„ã—ã¦ãã ã•ã„","äººå·¥çŸ¥èƒ½ã¯...","AIã¯..."
```

```bash
# ã‚«ã‚¹ã‚¿ãƒ CSVãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
python3 datasets/prepare_custom_dataset.py \
    --model meta-llama/Llama-2-7b-hf \
    --source csv \
    --source-path ./data.csv \
    --format alpaca
```

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®é¸æŠ

```bash
# Alpacaå½¢å¼ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
--format alpaca

# ChatMLå½¢å¼ï¼ˆGPTç³»ãƒ¢ãƒ‡ãƒ«ç”¨ï¼‰
--format chatml

# Llama-2 Chatå½¢å¼
--format llama2
```

## SFTè¨“ç·´ã®å®Ÿè¡Œ

### åŸºæœ¬çš„ãªè¨“ç·´

```bash
# Step 1: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
python3 datasets/prepare_custom_dataset.py \
    --model meta-llama/Llama-2-7b-hf \
    --source alpaca \
    --num-samples 10000

# Step 2: SFTè¨“ç·´å®Ÿè¡Œ
python3 scripts/train_sft.py \
    --model-name meta-llama/Llama-2-7b-hf \
    --dataset-path datasets/alpaca_Llama-2-7b-hf_10000 \
    --output-dir ./outputs/llama2-7b-alpaca \
    --num-epochs 3 \
    --batch-size 4 \
    --learning-rate 2e-4 \
    --use-lora \
    --gradient-checkpointing \
    --bf16
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡çš„ãªè¨“ç·´ï¼ˆLoRAï¼‰

```bash
# LoRAã‚’ä½¿ç”¨ã—ãŸãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªè¨“ç·´
python3 scripts/train_sft.py \
    --model-name meta-llama/Llama-2-7b-hf \
    --dataset-path datasets/alpaca_Llama-2-7b-hf_10000 \
    --output-dir ./outputs/llama2-7b-lora \
    --use-lora \
    --lora-r 16 \
    --lora-alpha 32 \
    --lora-dropout 0.1 \
    --lora-target-modules q_proj,k_proj,v_proj,o_proj \
    --batch-size 8 \
    --gradient-accumulation-steps 2 \
    --bf16
```

### é‡å­åŒ–ã‚’ä½¿ç”¨ã—ãŸè¨“ç·´ï¼ˆå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ç”¨ï¼‰

```bash
# 4-bité‡å­åŒ– + LoRAï¼ˆ70Bãƒ¢ãƒ‡ãƒ«ã§ã‚‚1GPUå¯èƒ½ï¼‰
python3 scripts/train_sft.py \
    --model-name meta-llama/Llama-2-70b-hf \
    --dataset-path datasets/alpaca_Llama-2-70b-hf_10000 \
    --output-dir ./outputs/llama2-70b-qlora \
    --use-4bit \
    --use-lora \
    --batch-size 1 \
    --gradient-accumulation-steps 16 \
    --bf16
```

### ãƒãƒ«ãƒãƒãƒ¼ãƒ‰åˆ†æ•£è¨“ç·´

```bash
# DeepSpeedã‚’ä½¿ç”¨ã—ãŸ8ãƒãƒ¼ãƒ‰è¨“ç·´
deepspeed --num_nodes 8 \
    --num_gpus 64 \
    --hostfile hostfile \
    scripts/train_sft.py \
    --model-name meta-llama/Llama-2-7b-hf \
    --dataset-path datasets/alpaca_Llama-2-7b-hf_10000 \
    --output-dir ./outputs/llama2-7b-distributed \
    --deepspeed configs/ds_config_7b.json \
    --batch-size 16 \
    --gradient-accumulation-steps 1 \
    --bf16
```

## ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ

### å®Œå…¨ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ

```bash
# Step 1: Hugging Faceèªè¨¼
./setup/configure_huggingface.sh

# Step 2: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ï¼ˆè¤‡æ•°ã‚µã‚¤ã‚ºï¼‰
for size in 1000 5000 10000; do
    python3 datasets/prepare_custom_dataset.py \
        --model meta-llama/Llama-2-7b-hf \
        --source alpaca \
        --num-samples $size
done

# Step 3: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
for nodes in 2 4 8; do
    echo "Testing with $nodes nodes..."
    ./scripts/run_benchmark.sh meta-llama/Llama-2-7b-hf $nodes
done

# Step 4: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
python3 scripts/generate_report.py results/
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š

```bash
# GPUåˆ©ç”¨ç‡ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
watch -n 1 'nvidia-smi --query-gpu=utilization.gpu,memory.used --format=csv'

# è¨“ç·´é€Ÿåº¦ã®ç¢ºèª
tail -f outputs/*/trainer_state.json | grep loss

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
./scripts/collect_metrics.sh results/current/
```

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

#### 1. Hugging Faceãƒ¢ãƒ‡ãƒ«ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ããªã„

```bash
# ãƒˆãƒ¼ã‚¯ãƒ³ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
echo $HF_TOKEN

# å†è¨­å®š
source ~/.hf_token

# ãƒ¢ãƒ‡ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã®ãƒ†ã‚¹ãƒˆ
python3 -c "
from transformers import AutoModel
import os
model = AutoModel.from_pretrained('meta-llama/Llama-2-7b-hf', 
                                  token=os.environ.get('HF_TOKEN'))
"
```

#### 2. Out of Memory (OOM)ã‚¨ãƒ©ãƒ¼

```bash
# è§£æ±ºç­–1: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™
--batch-size 2
--gradient-accumulation-steps 8

# è§£æ±ºç­–2: LoRAã‚’ä½¿ç”¨
--use-lora
--lora-r 8  # rankã‚’ä¸‹ã’ã‚‹

# è§£æ±ºç­–3: é‡å­åŒ–ã‚’ä½¿ç”¨
--use-8bit  # ã¾ãŸã¯ --use-4bit

# è§£æ±ºç­–4: gradient checkpointingã‚’æœ‰åŠ¹åŒ–
--gradient-checkpointing
```

#### 3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¨ãƒ©ãƒ¼

```bash
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¤œè¨¼
python3 -c "
from datasets import load_from_disk
ds = load_from_disk('datasets/alpaca_Llama-2-7b-hf_10000')
print(f'Train samples: {len(ds[\"train\"])}')
print(f'Columns: {ds[\"train\"].column_names}')
print(f'First sample: {ds[\"train\"][0]}')
"
```

#### 4. è¨“ç·´ãŒé…ã„

```bash
# Flash Attention 2ã‚’æœ‰åŠ¹åŒ–
--use-flash-attention

# Mixed precisionã‚’ç¢ºèª
--bf16  # H200ã§ã¯æ¨å¥¨

# DataLoaderãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’å¢—ã‚„ã™
export OMP_NUM_THREADS=4
```

### ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰

```bash
# å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§å‹•ä½œç¢ºèª
python3 datasets/prepare_custom_dataset.py \
    --model gpt2 \
    --source alpaca \
    --num-samples 100

python3 scripts/train_sft.py \
    --model-name gpt2 \
    --dataset-path datasets/alpaca_gpt2_100 \
    --output-dir ./test_output \
    --num-epochs 1 \
    --batch-size 2 \
    --logging-steps 1 \
    --save-steps 10
```

## ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### æ¨å¥¨è¨­å®š

| ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º | ãƒãƒƒãƒã‚µã‚¤ã‚º | LoRA rank | é‡å­åŒ– | ãƒãƒ¼ãƒ‰æ•° |
|------------|-------------|-----------|--------|---------|
| 7B | 4-8 | 16 | ãªã— | 1-2 |
| 13B | 2-4 | 16 | 8-bit | 2-4 |
| 70B | 1 | 8 | 4-bit | 8 |

### è¨“ç·´ã®ã‚³ãƒ„

1. **æ®µéšçš„ãªã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—**
   - ã¾ãšå°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§å‹•ä½œç¢ºèª
   - å¾ã€…ã«ãƒ‡ãƒ¼ã‚¿é‡ã¨ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã™

2. **ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ´»ç”¨**
   - `--save-steps`ã‚’é©åˆ‡ã«è¨­å®š
   - ä¸­æ–­ã—ã¦ã‚‚å†é–‹å¯èƒ½

3. **ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–**
   - è¨“ç·´æå¤±ã®æ¨ç§»ã‚’ç¢ºèª
   - GPUåˆ©ç”¨ç‡ã‚’90%ä»¥ä¸Šã«ç¶­æŒ

4. **ãƒ‡ãƒ¼ã‚¿å“è³ª**
   - é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨
   - é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®é™¤å»
   - é©åˆ‡ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆé¸æŠ

## ã¾ã¨ã‚

ã“ã®ã‚¬ã‚¤ãƒ‰ã«å¾“ãˆã°ã€H200ã‚¯ãƒ©ã‚¹ã‚¿ã§åŠ¹ç‡çš„ã«LLMã®SFTã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚
è³ªå•ã‚„å•é¡ŒãŒã‚ã‚‹å ´åˆã¯ã€GitHubã‚¤ã‚·ãƒ¥ãƒ¼ã¾ãŸã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

---
æœ€çµ‚æ›´æ–°: 2025å¹´8æœˆ